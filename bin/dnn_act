#! /usr/bin/env python

"""
Extract activation from DNN
"""

import sys
import time
import h5py
import argparse
import numpy as np

from torch.utils.data import DataLoader
from torchvision import transforms
from dnnbrain.dnn.io import NetLoader, PicDataset, VidDataset
from dnnbrain.utils.io import read_stim_csv
from dnnbrain.dnn.analyzer import dnn_activation


def main():
    parser = argparse.ArgumentParser(description='Extract activation from DNN')
    parser.add_argument('-net',
                        metavar='Net',
                        required=True,
                        type=str,
                        help='a neural network name')
    parser.add_argument('-layer',
                        metavar='Layer',
                        required=True,
                        type=str,
                        nargs='+',
                        help="names of the layers used to specify where activation is extracted from "
                             "For example, 'conv1' represents the first convolution layer, and "
                             "'fc1' represents the first full connection layer. ")
    parser.add_argument('-stim',
                        metavar='Stimulus',
                        required=True,
                        type=str,
                        help='a .stim.csv file which contains stimulus information')
    parser.add_argument('-out',
                        metavar='Output',
                        required=True,
                        type=str,
                        help='an output filename with suffix .act.h5')
    args = parser.parse_args()
    assert args.out.endswith('.act.h5'), "the output file's suffix must be .act.h5"

    # -load DNN-
    net_loader = NetLoader(args.net)
    transform = transforms.Compose([transforms.Resize(net_loader.img_size), transforms.ToTensor()])

    # -load stimuli-
    stim_dict = read_stim_csv(args.stim)
    if stim_dict['type'] == 'picture':
        dataset = PicDataset(stim_dict['path'], stim_dict['stim']['stimID'],
                             stim_dict['stim'].get('condition', None), transform)
    elif stim_dict['type'] == 'video':
        dataset = VidDataset(stim_dict['path'], stim_dict['stim']['stimID'],
                             stim_dict['stim'].get('condition', None), transform)
    else:
        raise TypeError('{} is not a supported stimulus type.'.format(stim_dict['type']))
    data_loader = DataLoader(dataset, batch_size=8, shuffle=False)

    # -extract activation-
    n_stim = len(dataset)
    wf = h5py.File(args.out, 'w')
    for layer in args.layer:
        dnn_acts = []
        for stims, _ in data_loader:
            dnn_acts.extend(dnn_activation(stims, net_loader.model, net_loader.layer2keys[layer]))
            print('Extracted acts: {0}/{1}'.format(len(dnn_acts), n_stim))
        dnn_acts = np.array(dnn_acts)
        raw_shape = dnn_acts.shape
        dnn_acts = dnn_acts.reshape((raw_shape[0], raw_shape[1], -1))

        # write to file
        dset = wf.create_dataset(layer, data=dnn_acts)
        dset.attrs['raw_shape'] = raw_shape

    # create some information
    wf.attrs['title'] = args.net + ' act'
    wf.attrs['cmd'] = ' '.join(sys.argv)
    wf.attrs['date'] = time.asctime()
    wf.close()


if __name__ == '__main__':
    main()
