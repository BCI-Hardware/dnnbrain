#! /usr/bin/env python

"""
Extract activation from DNN
"""

import argparse
import numpy as np

from torch.utils.data import DataLoader
from torchvision import transforms
from dnnbrain.dnn.core import DNN, Stimulus, Mask
from dnnbrain.dnn.base import ImageSet, VideoSet


def main():
    parser = argparse.ArgumentParser(description=__doc__)
    parser.add_argument('-net',
                        metavar='Net',
                        required=True,
                        type=str,
                        help='a neural network name')
    parser.add_argument('-layer',
                        metavar='Layer',
                        type=str,
                        nargs='+',
                        help="names of the layers used to specify where activation is extracted from "
                             "For example, 'conv1' represents the first convolution layer, and "
                             "'fc1' represents the first full connection layer. ")
    parser.add_argument('-dmask',
                        metavar='DnnMask',
                        type=str,
                        help='a .dmask.csv file in which layers of interest are listed '
                             'with their own channels or columns of interest.')
    parser.add_argument('-stim',
                        metavar='Stimulus',
                        required=True,
                        type=str,
                        help='a .stim.csv file which contains stimulus information')
    parser.add_argument('-out',
                        metavar='Output',
                        required=True,
                        type=str,
                        help='an output filename with suffix .act.h5')
    args = parser.parse_args()
    assert np.logical_xor(args.layer is None, args.dmask is None), \
        'Use one and only one of the -layer and -dmask!'

    # -load DNN-
    dnn = DNN(args.net)

    # -load stimuli-
    stim = Stimulus(args.stim)
    transform = transforms.Compose([transforms.Resize(dnn.img_size), transforms.ToTensor()])
    if stim.meta['type'] == 'image':
        dataset = ImageSet(stim.meta['path'], stim.get('stimID'), transform=transform)
    elif stim.meta['type'] == 'video':
        dataset = VideoSet(stim.meta['path'], stim.get('stimID'), transform=transform)
    else:
        raise TypeError('{} is not a supported stimulus type.'.format(stim.meta['type']))
    batch_size = 8
    data_loader = DataLoader(dataset, batch_size, shuffle=False)

    # -load mask-
    dmask = Mask()
    if args.layer is not None:
        for layer in args.layer:
            dmask.set(layer)
    else:
        dmask.load(args.dmask)

    # -extract activation-
    n_stim = len(dataset)
    act = []
    for stims, _ in data_loader:
        act.append(dnn.compute_activation(stims, dmask))
        count = len(act) * batch_size
        print('Extracted acts: {0}/{1}'.format(min(count, n_stim), n_stim))
    act = act[0].concatenate(act[1:])

    act.save(args.out)


if __name__ == '__main__':
    main()
