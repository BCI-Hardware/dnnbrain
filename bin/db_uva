#! /usr/bin/env python

"""
Use CNN activation to predict brain activation
"""

import os
import time
import argparse
import numpy as np
import pandas as pd
import pickle as pkl

from torch.utils.data import DataLoader
from torchvision import transforms
from os.path import join as pjoin
from dnnbrain.dnn.analyzer import dnn_activation, convolve_hrf
from dnnbrain.dnn.io import NetLoader, read_dnn_csv, PicDataset, VidDataset
from dnnbrain.brain.io import load_brainimg, save_brainimg
from sklearn.linear_model import LinearRegression, LogisticRegression
from sklearn.model_selection import cross_val_score
from sklearn.metrics import pairwise_distances
    

def main():
    parser = argparse.ArgumentParser(description='Use DNN activation to predict responses of brain or behavior '
                                                 'by univariate model.')
    parser.add_argument('-net',
                        type=str,
                        required=True,
                        metavar='NetName',
                        help='neural network name')
    parser.add_argument('-layer',
                        type=str,
                        required=True,
                        metavar='LayerName',
                        help="The name of the layer whose activation is used to predict brain activity. "
                             "For example, 'conv1' represents the first convolution layer, and "
                             "'fc1' represents the first full connection layer.")
    parser.add_argument('-dmask',
                        type=str,
                        required=False,
                        metavar='DnnMaskFile',
                        help='a db.csv file in which channles and columns of interest ae listed.')
    parser.add_argument('-iteraxis',
                        type=str,
                        metavar='Axis',
                        choices=['layer', 'channel', 'column'],
                        default='layer',
                        help="Iterate alone the specified axis."
                             "'channel': Summarize the maximal prediction score for each channel. "
                             "'column': Summarize the maximal prediction score for each channel. "
                             "Default is layer: Summarize the maximal prediction score for the whole layer.")
    parser.add_argument('-dfe',
                        type=str,
                        metavar='DnnFeatureExtraction',
                        choices=('max', 'mean', 'median'),
                        help='Feature extraction for each iterated DNN activation. '
                             'max: use maximal activation as feature; '
                             'mean: use mean activation as feature; '
                             'median: use median activation as feature.')
    parser.add_argument('-stim',
                        type=str,
                        required=True,
                        metavar='StimuliInfoFile',
                        help='a db.csv file which contains stimuli information')
    parser.add_argument('-hrf',
                        action='store_true',
                        help='Convolute dnn activation with SPM canonical hemodynamic response function. '
                             'And match it with the time points of Brain activation.')
    parser.add_argument('-resp',
                        type=str,
                        required=True,
                        metavar='Response',
                        help='A file contains responses used to be predicted by dnn activation. '
                             'If the file ends with .db.csv, usually means roi-wise or behavior data. '
                             'If the file is nifti/cifti file, usually means voxel/vertex-wise data.')
    parser.add_argument('-bmask',
                        type=str,
                        metavar='MaskFile',
                        help='Brain mask is used to extract activation locally. '
                             'Only used when the response file is nifti/cifti file.')
    parser.add_argument('-model',
                        type=str,
                        required=True,
                        metavar='BrainModel',
                        choices=['glm', 'lrc', 'corr_r'],
                        help='Select a model to predict brain or behavior responses by dnn activation. '
                             'Use glm (general linear model) for regression analysis. '
                             'Use lrc (logistic regression) for classification analysis.'
                             'Use corr (correlation) to calculate R square.')
    parser.add_argument('-cvfold',
                        type=int,
                        metavar='FoldNumber',
                        default=3,
                        help='cross validation fold number'
                             'This option will be ignored if -model is corr_r.')
    parser.add_argument('-out',
                        type=str,
                        required=True,
                        metavar='OutputDir',
                        help='output directory. Output data will stored as the following directory: '
                             'layer/[channel/column]/max_score, max_position')
    args = parser.parse_args()

    # -Load response start-
    if args.resp.endswith('.db.csv'):
        resp_dict = read_dnn_csv(args.resp)
        Y = np.array(list(resp_dict['var'].values())).T

    elif args.resp.endswith('.nii') or args.resp.endswith('.nii.gz'):
        Y, header = load_brainimg(args.resp)
        bshape = Y.shape[1:]

        # Get resp data within brain mask
        if args.bmask is None:
            bmask = np.any(Y, 0)
        else:
            bmask, _ = load_brainimg(args.bmask, ismask=True)
            assert bshape == bmask.shape, 'brain mask and brain response mismatched in space'
            bmask = bmask.astype(np.bool)
        Y = Y[:, bmask]

    else:
        raise IOError('Only .db.csv and nifti/cifti are supported')
    n_samp, n_meas = Y.shape  # n_sample x n_measures
    print('Finish loading response: ', args.resp)
    # -Load response end-

    # -Extract DNN activation start-
    start1 = time.time()
    # --Load DNN--
    net_loader = NetLoader(args.net)
    transform = transforms.Compose([transforms.Resize(net_loader.img_size), transforms.ToTensor()])

    # --Load stimuli--
    stim_dict = read_dnn_csv(args.stim)
    is_crop = stim_dict.get('crop', 'False')
    if is_crop == 'True':
        crops = np.array([stim_dict['var']['left_coord'], stim_dict['var']['upper_coord'],
                          stim_dict['var']['right_coord'], stim_dict['var']['lower_coord']]).T
    else:
        crops = None
    if stim_dict['stimType'] == 'picture':
        dataset = PicDataset(stim_dict['stimPath'], stim_dict['var']['stimID'],
                             stim_dict['var'].get('condition', None), transform, crops)
    elif stim_dict['stimType'] == 'video':
        dataset = VidDataset(stim_dict['stimPath'], stim_dict['var']['stimID'],
                             stim_dict['var'].get('condition', None), transform, crops)
    else:
        raise TypeError('{} is not a supported stimulus type.'.format(stim_dict['stimType']))
    data_loader = DataLoader(dataset, batch_size=100, shuffle=False)

    # --Extract activation--
    print('Layer: {0}_{1}\nstimPath: {2}'.format(args.net, args.layer, stim_dict['stimPath']))

    # ---DNN activation's feature extraction---
    if args.dfe is None:
        fe_axis, fe_meth = None, None
    else:
        if args.iteraxis == 'layer':
            fe_axis = 'layer'
        elif args.iteraxis == 'channel':
            fe_axis = 'column'
        else:
            fe_axis = 'channel'
        fe_meth = args.dfe

    # ---DNN activation mask---
    if args.dmask is None:
        X = dnn_activation(data_loader, args.net, args.layer, fe_axis=fe_axis, fe_meth=fe_meth)
    else:
        dmask = read_dnn_csv(args.dmask)
        print('dmask: ', args.dmask)
        X = dnn_activation(data_loader, args.net, args.layer, dmask['var'].get('chn', None),
                           dmask['var'].get('col', None), fe_axis=fe_axis, fe_meth=fe_meth)
    n_stim, n_chn, n_col = X.shape
    end1 = time.time()
    print('Finish extracting DNN activaton: cost {} seconds'.format(end1 - start1))
    # -Extract DNN activation end-

    # transpose axis to keep X's shape as (n_stimulus, n_iterator, n_element)
    if args.iteraxis == 'layer':
        X = X.reshape(n_stim, 1, n_chn * n_col)
    elif args.iteraxis == 'column':
        X = X.transpose(0, 2, 1)
    n_stim, n_iter, n_elem = X.shape

    # Convolve with HRF
    if args.hrf:
        start2 = time.time()
        onset = stim_dict['var']['onset']
        duration = stim_dict['var']['duration']
        tr = float(stim_dict['hrf_tr'])
        ops = int(stim_dict.get('hrf_ops', 100))
        X = convolve_hrf(X.reshape(n_stim, -1), onset, duration, n_samp, tr, ops)
        X = X.reshape(n_samp, n_iter, n_elem)  # cover X before HRFed, save memory
        end2 = time.time()
        print('Finish HRF convolution: cost {} seconds'.format(end2 - start2))

    # -Do prediction start-
    start3 = time.time()
    # --Prepare model--
    if args.model == 'lrc':
        model = LogisticRegression()
        score_evl = 'accuracy'
    else:
        model = LinearRegression()
        score_evl = 'explained_variance'
    print('Model:', args.model)

    # --Perform univariate prediction analysis--
    # prepare container
    score_arr = np.zeros((n_iter, n_meas), dtype=np.float)
    if args.dfe is None:
        # Channel and column positions only exist when args.dfe is None
        channel_arr = np.zeros_like(score_arr, dtype=np.int)
        column_arr = np.zeros_like(score_arr, dtype=np.int)
    model_arr = np.zeros_like(score_arr, dtype=np.object)

    # start iteration
    for meas_idx in range(n_meas):
        for iter_idx in range(n_iter):
            if args.model == 'corr_r':
                score_tmp = pairwise_distances(X[:, iter_idx, :].T, Y[:, meas_idx].reshape(1, -1), 'correlation')
                score_tmp = (1 - np.squeeze(score_tmp)) ** 2
            else:
                score_tmp = []
                for elem_idx in range(n_elem):
                    cv_scores = cross_val_score(model, X[:, iter_idx, elem_idx][:, np.newaxis],
                                                Y[:, meas_idx], scoring=score_evl, cv=args.cvfold)
                    score_tmp.append(np.mean(cv_scores))

            # find max score
            max_elem_idx = np.argmax(score_tmp)
            max_score = score_tmp[max_elem_idx]
            score_arr[iter_idx, meas_idx] = max_score

            # find position for the max score
            if args.dfe is None:
                if args.iteraxis == 'layer':
                    chn_idx = max_elem_idx // n_col
                    col_idx = max_elem_idx % n_col
                elif args.iteraxis == 'channel':
                    chn_idx, col_idx = iter_idx, max_elem_idx
                else:
                    chn_idx, col_idx = max_elem_idx, iter_idx

                if args.dmask is None:
                    chn_pos = chn_idx + 1
                    col_pos = col_idx + 1
                else:
                    chn_pos = dmask['var']['chn'][chn_idx] + 1
                    col_pos = dmask['var']['col'][col_idx] + 1

                channel_arr[iter_idx, meas_idx] = chn_pos
                column_arr[iter_idx, meas_idx] = col_pos

            # fit the max-score model
            model_arr[iter_idx, meas_idx] = model.fit(X[:, iter_idx, max_elem_idx][:, np.newaxis], Y[:, meas_idx])
            print('Measure{0}/{1}: finish iteration{2}/{3}'.format(meas_idx+1, n_meas, iter_idx+1, n_iter))
    end3 = time.time()
    print('Finish prediction: cost {} seconds'.format(end3 - start3))
    # -Do prediction end-

    # -save out start-
    # prepare directory
    trg_dir = pjoin(args.out, '{0}_{1}'.format(args.net, args.layer))
    if not os.path.isdir(trg_dir):
        os.makedirs(trg_dir)
    if args.iteraxis != 'layer':
        trg_dir = pjoin(trg_dir, args.iteraxis)
        if not os.path.isdir(trg_dir):
            os.makedirs(trg_dir)

    # save files
    if args.resp.endswith('.db.csv'):
        # save score
        score_df = pd.DataFrame(score_arr, columns=resp_dict['var'].keys())
        score_df.to_csv(pjoin(trg_dir, 'max_score.csv'), index=False)
        del score_arr  # save memory

        # save position
        if args.dfe is None:
            chn_pos_df = pd.DataFrame(channel_arr, columns=resp_dict['var'].keys())
            chn_pos_df.to_csv(pjoin(trg_dir, 'chn_position.csv'), index=False)
            del channel_arr  # save memory
            col_pos_df = pd.DataFrame(column_arr, columns=resp_dict['var'].keys())
            col_pos_df.to_csv(pjoin(trg_dir, 'col_position.csv'), index=False)
            del column_arr  # save memory

    elif args.resp.endswith('.nii') or args.resp.endswith('.nii.gz'):
        resp_suffix = '.'.join(args.resp.split('.')[1:])

        # save score
        score_img = np.zeros((n_iter, *bshape))
        score_img[:, bmask] = score_arr
        save_brainimg(pjoin(trg_dir, 'max_score.' + resp_suffix), score_img, header)
        del score_arr  # save memory

        # save position
        if args.dfe is None:
            chn_pos_img = np.zeros_like(score_img, dtype=np.int)
            chn_pos_img[:, bmask] = channel_arr
            save_brainimg(pjoin(trg_dir, 'chn_position.' + resp_suffix), chn_pos_img, header)
            del channel_arr  # save memory
            col_pos_img = np.zeros_like(score_img, dtype=np.int)
            col_pos_img[:, bmask] = column_arr
            save_brainimg(pjoin(trg_dir, 'col_position.' + resp_suffix), col_pos_img, header)
            del column_arr  # save memory

    pkl.dump(model_arr, open(pjoin(trg_dir, 'model.pkl'), 'wb'))
    # -save out end-


if __name__ == '__main__':
    main()
